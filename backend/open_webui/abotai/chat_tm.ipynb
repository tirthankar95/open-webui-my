{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv \n",
    "import os \n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Functional']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 27017\n",
    "client = MongoClient(f\"mongodb://{host}:{port}/\")\n",
    "db = client[\"LLMQueryAgent\"]\n",
    "print(db.list_collection_names())\n",
    "collection = db[\"Functional\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"You are an expert AI assistant specializing in generating efficient and accurate MongoDB queries.A database entry is as follows: {'_id': ObjectId('67ced2ca97c2a94c1b06d3d1'), 'frame_time': datetime.datetime(2024, 11, 4, 12, 5, 5, 428000), 'frame_time_epoch': 1730721905428675000, 'frame_number': '1496', 'protocol': 'HTTP2/JSON', 'src': 'UDR', 'dst': 'UDM', 'tcp_srcport': '2760', 'tcp_dstport': '34332', 'udp_srcport': '', 'udp_dstport': '', 'sctp_srcport': '', 'sctp_dstport': '', 'info': 'HEADERS[21]: /nudr-dr/v2/subscription-data/imsi-912116000000001/context-data/smf-registrations, 404', 'Error_Markers': {'type': 'http2', 'status': '404', 'request': '/nudr-dr/v2/subscription-data/imsi-912116000000001/context-data/smf-registrations', 'method': 'get'}, 'Message_Identifier': {'message': 'HEADERS[21]: /nudr-dr/v2/subscription-data/imsi-912116000000001/context-data/smf-registrations, 404'}, 'job_id': '98ad489a-9158-44f8-8f04-7b7e362e0d74'}.\\nHere is an example of mongo query interaction you must follow this assisstant response: dummy.\\nWhen returning a query, output only the query itself with no extra words or explanations or punctuations,so that an user can directly run the command.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are an expert AI assistant specializing in generating efficient and accurate MongoDB queries.\"\"\" + \\\n",
    "                   \"\"\"A database entry is as follows: {schema}.\\nHere is an example of mongo query interaction you must follow this assisstant response: {one_shot}.\\n\"\"\" + \\\n",
    "                   \"\"\"When returning a query, output only the query itself with no extra words or explanations or punctuations,\"\"\" + \\\n",
    "                   \"\"\"so that an user can directly run the command.\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"user\", \"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt.invoke(\n",
    "    {\n",
    "        \"schema\": collection.find_one({}),\n",
    "        \"one_shot\": \"dummy\",\n",
    "        \"history\": [\n",
    "            {\"role\": \"user\", \"content\": \"\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ],\n",
    "        \"query\": \"What is the capital of France?\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# docker run -it -p 8000:8000  tirthankar95/qwen7b\n",
    "\n",
    "# llama-server -m qwen2.5-7b-instruct-q4_0.gguf \\\n",
    "#     --threads -1 --host 0.0.0.0 --port 8000 \\\n",
    "#     --log-file chat_llm \\\n",
    "#     --ctx-size 8192 \\\n",
    "#     --n-predict 512 \\\n",
    "#     --temp 0.5 \\\n",
    "#     --top-k 10 \\\n",
    "#     --top-p 0.9 \\\n",
    "#     --min-p 0.1 \\\n",
    "#     --repeat-penalty 1.1 \\\n",
    "#     --mlock \\\n",
    "#     --batch-size 16\n",
    "\n",
    "openai_api_key = \"NA\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "model_name = \"qwen2.5-7b-instruct-q4_0.gguf\"\n",
    "model = ChatOpenAI(\n",
    "    api_key = openai_api_key,\n",
    "    base_url = openai_api_base,\n",
    "    model_name = model_name\n",
    ")\n",
    "model_gpt = ChatOpenAI(\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    model_name = \"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def mongo_exec(ai_message):\n",
    "    try:\n",
    "        print(f'AI mongo query: {ai_message}')\n",
    "        return f\"The result of the query is: {eval(ai_message)}.\"\n",
    "    except Exception as e:\n",
    "        pattern = re.compile(r\"\\bcollection\")\n",
    "        if pattern.search(ai_message):\n",
    "            return f\"Extract only the query from {ai_message}.\"\n",
    "        else:\n",
    "            return f\"Invalid query. Please try again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "import os \n",
    "\n",
    "# Get model and tokenizer.\n",
    "local_dir = \"./embed_model/\"\n",
    "repo_id = \"thenlper/gte-base\"\n",
    "filenames = list_repo_files(repo_id)\n",
    "for file in filenames:\n",
    "    if not os.path.exists(os.path.join(local_dir, file)):\n",
    "        hf_hub_download(repo_id, file, local_dir=local_dir)\n",
    "\n",
    "# Load embedding model.\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(model_name = repo_id, \\\n",
    "                           model_kwargs = model_kwargs, \\\n",
    "                           encode_kwargs = encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='d0439792-3466-4eb2-bbdb-09cab9a09867', metadata={'source': 'manual_tmittra'}, page_content='user: Count the number of errors with source as UDR?\\n assistant:collection.count_documents({\"src\": \"UDR\"})')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from uuid import uuid4\n",
    "\n",
    "# Save one shot example in mongo-db\n",
    "# DB_NAME = \"LLM_MONGO\"\n",
    "# COLLECTION_NAME = \"ONE_SHOT_EXAMPLES\"\n",
    "# ATLAS_VECTOR_SEARCH_INDEX_NAME = \"LLM_MONGO_INDEX\" \n",
    "# one_shot_collection = client[DB_NAME][COLLECTION_NAME]\n",
    "# # Chat 1\n",
    "# chat_example_1 = \"user: Count the number of errors with source as UDR ?\\n\" + \\\n",
    "#                  \"\"\"ai: collection.count_documents({'src': 'AMF'})\"\"\"\n",
    "# chat1 = {\n",
    "#     \"_id\": 1,\n",
    "#     \"one_shot\": chat_example_1\n",
    "# }\n",
    "# one_shot_collection.insert_one(chat1)\n",
    "\n",
    "# Chroma DB \n",
    "DB_NAME = \"LLM_MONGO_1\"\n",
    "COLLECTION_NAME = \"ONE_SHOT_EXAMPLES\"\n",
    "vector_store = Chroma(collection_name = COLLECTION_NAME, \\\n",
    "                      embedding_function = hf, \\\n",
    "                      persist_directory = f\"./{DB_NAME}\")\n",
    "\"\"\"\n",
    "Make all examples to be UDR, so that similarity search is better.\n",
    "Otherwise, similarity search may return a one-shot if only the node name matches.\n",
    "\"\"\"\n",
    "examples = [\n",
    "    Document(\n",
    "        page_content = \"\"\"user: Count the number of errors with source as UDR?\\n assistant:collection.count_documents({\"src\": \"UDR\"})\"\"\",\n",
    "        metadata = {\"source\": \"manual_tmittra\"}),\n",
    "    Document(\n",
    "        page_content = \"\"\"user: Display errors with destination as UDR?\\nassistant: list(collection.find({\"dst\": \"UDR\"}))\"\"\",\n",
    "        metadata = {\"source\": \"manual_tmittra\"})\n",
    "    ]\n",
    "uuids = [str(uuid4()) for _ in range(len(examples))]\n",
    "vector_store.add_documents(documents = examples, ids = uuids)\n",
    "retriver = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "print(retriver.invoke(\"Count the number of errors with source as AMF?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schema': {'_id': ObjectId('67ced2ca97c2a94c1b06d3d1'),\n",
       "  'frame_time': datetime.datetime(2024, 11, 4, 12, 5, 5, 428000),\n",
       "  'frame_time_epoch': 1730721905428675000,\n",
       "  'frame_number': '1496',\n",
       "  'protocol': 'HTTP2/JSON',\n",
       "  'src': 'UDR',\n",
       "  'dst': 'UDM',\n",
       "  'tcp_srcport': '2760',\n",
       "  'tcp_dstport': '34332',\n",
       "  'udp_srcport': '',\n",
       "  'udp_dstport': '',\n",
       "  'sctp_srcport': '',\n",
       "  'sctp_dstport': '',\n",
       "  'info': 'HEADERS[21]: /nudr-dr/v2/subscription-data/imsi-912116000000001/context-data/smf-registrations, 404',\n",
       "  'Error_Markers': {'type': 'http2',\n",
       "   'status': '404',\n",
       "   'request': '/nudr-dr/v2/subscription-data/imsi-912116000000001/context-data/smf-registrations',\n",
       "   'method': 'get'},\n",
       "  'Message_Identifier': {'message': 'HEADERS[21]: /nudr-dr/v2/subscription-data/imsi-912116000000001/context-data/smf-registrations, 404'},\n",
       "  'job_id': '98ad489a-9158-44f8-8f04-7b7e362e0d74'},\n",
       " 'one_shot': 'user: Count the number of errors with source as UDR?\\n assistant:collection.count_documents({\"src\": \"UDR\"})',\n",
       " 'history': [{'role': 'user',\n",
       "   'content': 'Count the number of errors with destination as GNB?'},\n",
       "  {'role': 'assistant', 'content': 'The result of the query is: 2.'}],\n",
       " 'query': 'Count the number of errors with source as AMF?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vector_retriver(input_2llm):\n",
    "    query, history = input_2llm['query'], input_2llm['history']\n",
    "    rdocs = retriver.invoke(query)\n",
    "    fmt_docx = \"\\n\".join([doc.page_content for doc in rdocs])\n",
    "    return {\"schema\": collection.find_one({}), \\\n",
    "            \"one_shot\": fmt_docx, \\\n",
    "            \"history\": history, \\\n",
    "            \"query\": query}\n",
    "\n",
    "dummy_history = [{'role': 'user', 'content': 'Count the number of errors with destination as GNB?'}, \\\n",
    "                 {'role': 'assistant', 'content': 'The result of the query is: 2.'}]\n",
    "vector_retriver({\"query\": \"Count the number of errors with source as AMF?\", \\\n",
    "                 \"history\": dummy_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI mongo query: collection.count_documents({\"dst\": \"GNB\"})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The result of the query is:\\n2.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Option 2: For extracting the number of errors.\n",
    "# pipeline = [\n",
    "#     {\n",
    "#         \"$match\": { \"src\": \"AMF\" } \n",
    "#     },\n",
    "#     {\n",
    "#         \"$count\": \"num_errors\"  # $count requires a string field name\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# result = list(collection.aggregate(pipeline))\n",
    "# print(result) \n",
    "prompt_result = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"The result of the user query was: {result_query}\")\n",
    "    ]\n",
    ")\n",
    "chain = vector_retriver | prompt | model_gpt | StrOutputParser() | mongo_exec \n",
    "chain.invoke(\"Count the number of errors with destination as GNB?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qwen2.5-7b-Instruct-Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI mongo query: collection.count_documents({\"dst\": \"GNB\"})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The result of the query is: 2.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "MIN_CHAT_HISTORY = 3\n",
    "def query_chain(query, history):\n",
    "    print(history)\n",
    "    history_sz = min(MIN_CHAT_HISTORY, len(history))\n",
    "    try:\n",
    "        chain_fn = vector_retriver | prompt | model_gpt | StrOutputParser() | mongo_exec \n",
    "        history_to_take = history[-history_sz:]\n",
    "        history_to_take = [{'role': hist['role'], 'content': hist['content']} for hist in history_to_take]\n",
    "        return chain_fn.invoke({\"query\": query, \"history\": history_to_take})\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Test \n",
    "query_chain(\"Count the number of errors with destination as GNB?\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "AI mongo query: list(collection.find({\"dst\": \"UDR\"}))\n",
      "[{'role': 'user', 'metadata': None, 'content': 'from langchain_core.output_parsers import StrOutputParser', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'The result of the query is: [].', 'options': None}]\n",
      "AI mongo query: list(collection.find({\"dst\": \"UDR\"}))\n",
      "[{'role': 'user', 'metadata': None, 'content': 'from langchain_core.output_parsers import StrOutputParser', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'The result of the query is: [].', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'from langchain_core.output_parsers import StrOutputParser', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'The result of the query is: [].', 'options': None}]\n",
      "AI mongo query: collection.count_documents({\"dst\": \"GNB\"})\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr \n",
    "\n",
    "gr.ChatInterface(query_chain, type = \"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "local_dir = \"./embed_model/\"\n",
    "repo_id = \"thenlper/gte-base\"\n",
    "\n",
    "# Load embedding model.\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(model_name = repo_id, \\\n",
    "                           model_kwargs = model_kwargs, \\\n",
    "                           encode_kwargs = encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='d0439792-3466-4eb2-bbdb-09cab9a09867', metadata={'source': 'manual_tmittra'}, page_content='user: Count the number of errors with source as UDR?\\n assistant:collection.count_documents({\"src\": \"UDR\"})')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "COLLECTION_NAME = \"ONE_SHOT_EXAMPLES\"\n",
    "DB_NAME = \"LLM_MONGO_1\"\n",
    "vector_store = Chroma(collection_name = COLLECTION_NAME, \\\n",
    "                      embedding_function = hf, \\\n",
    "                      persist_directory = f\"./{DB_NAME}\")\n",
    "\n",
    "retriver = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "rdocs = retriver.invoke(\"Count the number of errors with destination as GNB?\")\n",
    "fmt_docx = \"\\n\".join([doc.page_content for doc in rdocs])\n",
    "print(retriver.invoke(\"Count the number of errors with source as AMF?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
