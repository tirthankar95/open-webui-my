{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f10320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import numpy as np \n",
    "from uuid import uuid4\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d26308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from chain_mongo import Chain_Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6082846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 15:42:30,988 - INFO - models.py:31 - Model[gpt-4o-mini], Base_URL[http://localhost:9090/v1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 15:42:37,956 - INFO - config.py:54 - PyTorch version 2.6.0 available.\n",
      "2025-05-05 15:42:38,640 - INFO - SentenceTransformer.py:218 - Load pretrained SentenceTransformer: thenlper/gte-base\n"
     ]
    }
   ],
   "source": [
    "mongo_chain = Chain_Mongo(session_id = \"qwerty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6670974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 15:45:37,143 - INFO - _client.py:1025 - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-05-05 15:45:37,182 - INFO - _client.py:1025 - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 15:45:37,707 - INFO - _client.py:1025 - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-05-05 15:45:51,748 - INFO - chain_mongo.py:98 - -m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m-m\n",
      "\n",
      "2025-05-05 15:45:51,923 - INFO - chain_mongo.py:61 - one_shot: \n",
      "\n",
      "2025-05-05 15:45:51,923 - INFO - chain_mongo.py:62 - history: []\n",
      "\n",
      "2025-05-05 15:45:51,924 - INFO - chain_mongo.py:63 - query: Count the number of errors with destination as GNB?\n",
      "\n",
      "2025-05-05 15:45:52,057 - INFO - chain_mongo.py:61 - one_shot: \n",
      "\n",
      "2025-05-05 15:45:52,058 - INFO - chain_mongo.py:62 - history: []\n",
      "\n",
      "2025-05-05 15:45:52,060 - INFO - chain_mongo.py:63 - query: Count the number of errors with destination as GNB?\n",
      "\n",
      "2025-05-05 15:45:52,073 - INFO - _base_client.py:1089 - Retrying request to /chat/completions in 0.383925 seconds\n",
      "2025-05-05 15:45:52,460 - INFO - _base_client.py:1089 - Retrying request to /chat/completions in 0.804271 seconds\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py\", line 136, in handle_request\n",
      "    raise exc\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py\", line 86, in handle_request\n",
      "    self._send_request_headers(**kwargs)\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py\", line 144, in _send_request_headers\n",
      "    with map_exceptions({h11.LocalProtocolError: LocalProtocolError}):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.LocalProtocolError: Illegal header value b'Bearer '\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 993, in _request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.LocalProtocolError: Illegal header value b'Bearer '\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tirthankar-mittra/open-webui-my/backend/open_webui/abotai/chain_mongo.py\", line 116, in call_chain\n",
      "    resp = self.chain_fn.invoke({\"query\": query, \"history\": history_to_take})\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3025, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 307, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 843, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 683, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 908, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 925, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 859, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1280, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 957, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1017, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1095, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1017, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1095, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/openai/_base_client.py\", line 1027, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1661, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 857, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/gradio/chat_interface.py\", line 862, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tirthankar-mittra/open-webui-my/backend/open_webui/abotai/chain_mongo.py\", line 119, in call_chain\n",
      "    ai_message, error = str(ai_message_error).split('|')\n",
      "    ^^^^^^^^^^^^^^^^^\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(mongo_chain.call_chain, type = \"messages\").launch(share = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add6f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "588b85ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98b5b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 01:26:48,171 - INFO - models.py:31 - Model[gpt-4o-mini], Base_URL[http://localhost:9090/v1]\n",
      "2025-05-06 01:26:57,285 - INFO - config.py:54 - PyTorch version 2.6.0 available.\n",
      "2025-05-06 01:26:57,724 - INFO - SentenceTransformer.py:218 - Load pretrained SentenceTransformer: thenlper/gte-base\n"
     ]
    }
   ],
   "source": [
    "from chroma_db import ChromaDB\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "examples = [\n",
    "    Document(\n",
    "        page_content = \"\"\"human: Count the number of errors with source as UDR?\\n ai:self.collection.count_documents({\"src\": \"UDR\"})\"\"\",\n",
    "        metadata = {\"source\": \"manual_tmittra\"}),\n",
    "    Document(\n",
    "        page_content = \"\"\"human: Display errors with destination as UDR?\\nai: list(self.collection.find({\"dst\": \"UDR\"}))\"\"\",\n",
    "        metadata = {\"source\": \"manual_tmittra\"})\n",
    "]\n",
    "vector_store = ChromaDB(\"MALE_XY\", examples).vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ce7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4af4513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8faab66a-3a63-454a-b797-aefb9349d9c3', metadata={'source': 'manual_tmittra'}, page_content='human: Count the number of errors with source as UDR?\\n ai:self.collection.count_documents({\"src\": \"UDR\"})')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver.invoke(\"Count the number of errors with source as UDM?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
